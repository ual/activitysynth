{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating TOD distributions\n",
    "\n",
    "Estimates TOD distribution for each TOD category for both school and departure time. \n",
    "\n",
    "TO DO: \n",
    " - Keep adding more distributions to fit [avoid distributions with performance issues a.k.a too slow to run]\n",
    " - Generate validation grpahs [Data vs fitted distributions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from urbansim_templates import modelmanager as mm\n",
    "from urbansim_templates.models import LargeMultinomialLogitStep\n",
    "from urbansim_templates.models import SmallMultinomialLogitStep\n",
    "import orca\n",
    "import os; os.chdir('../')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import statsmodels as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define best distribution functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code reference: \n",
    "https://stackoverflow.com/questions/6620471/fitting-empirical-distribution-to-theoretical-ones-with-scipy-python\n",
    "'''\n",
    "\n",
    "def best_fit_distribution(data, bins=200, ax=None):\n",
    "    \"\"\"Model data by finding best fit distribution to data\"\"\"\n",
    "    # Get histogram of original data\n",
    "    y, x = np.histogram(data, bins=bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
    "\n",
    "    # Distributions to check\n",
    "#     DISTRIBUTIONS = [        \n",
    "#         st.alpha,st.anglit,st.arcsine,st.beta,st.betaprime,st.bradford,st.burr,st.cauchy,st.chi,st.chi2,st.cosine,\n",
    "#         st.dgamma,st.dweibull,st.erlang,st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.fatiguelife,st.fisk,\n",
    "#         st.foldcauchy,st.foldnorm,st.frechet_r,st.frechet_l,st.genlogistic,st.genpareto,st.gennorm,st.genexpon,\n",
    "#         st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.genhalflogistic,st.gilbrat,st.gompertz,st.gumbel_r,\n",
    "#         st.gumbel_l,st.halfcauchy,st.halflogistic,st.halfnorm,st.halfgennorm,st.hypsecant,st.invgamma,st.invgauss,\n",
    "#         st.invweibull,st.johnsonsb,st.johnsonsu,st.ksone,st.kstwobign,st.laplace,st.levy,st.levy_l,st.levy_stable,\n",
    "#         st.logistic,st.loggamma,st.loglaplace,st.lognorm,st.lomax,st.maxwell,st.mielke,st.nakagami,st.ncx2,st.ncf,\n",
    "#         st.nct,st.norm,st.pareto,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,st.rdist,st.reciprocal,\n",
    "#         st.rayleigh,st.rice,st.recipinvgauss,st.semicircular,st.t,st.triang,st.truncexpon,st.truncnorm,st.tukeylambda,\n",
    "#         st.uniform,st.vonmises,st.vonmises_line,st.wald,st.weibull_min,st.weibull_max,st.wrapcauchy\n",
    "#     ]\n",
    "    DISTRIBUTIONS = [\n",
    "        st.chi,st.chi2,st.cosine,\n",
    "        st.dgamma,st.expon,st.f,\n",
    "        st.gamma,\n",
    "        st.gumbel_l,st.halfnorm,st.halfgennorm,\n",
    "        st.logistic,\n",
    "        st.norm,st.pearson3,st.powerlaw,st.powerlognorm,st.powernorm,\n",
    "        st.uniform\n",
    "    ]\n",
    "\n",
    "    # Best holders\n",
    "    best_distribution = st.norm\n",
    "    best_params = (0.0, 1.0)\n",
    "    best_sse = np.inf\n",
    "\n",
    "    # Estimate distribution parameters from data\n",
    "    for distribution in DISTRIBUTIONS:\n",
    "\n",
    "        # Try to fit the distribution\n",
    "        try:\n",
    "            # Ignore warnings from data that can't be fit\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore')\n",
    "\n",
    "                # fit dist to data\n",
    "                params = distribution.fit(data)\n",
    "\n",
    "                # Separate parts of parameters\n",
    "                arg = params[:-2]\n",
    "                loc = params[-2]\n",
    "                scale = params[-1]\n",
    "\n",
    "                # Calculate fitted PDF and error with fit in distribution\n",
    "                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n",
    "                sse = np.sum(np.power(y - pdf, 2.0))\n",
    "\n",
    "                # if axis pass in add to plot\n",
    "                try:\n",
    "                    if ax:\n",
    "                        pd.Series(pdf, x).plot(ax=ax)\n",
    "                    end\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                # identify if this distribution is better\n",
    "                if best_sse > sse > 0:\n",
    "                    best_distribution = distribution\n",
    "                    best_params = params\n",
    "                    best_sse = sse\n",
    "\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (best_distribution.name, best_params)\n",
    "\n",
    "def make_pdf(dist, params, size=10000):\n",
    "    \"\"\"Generate distributions's Probability Distribution Function \"\"\"\n",
    "\n",
    "    # Separate parts of parameters\n",
    "    arg = params[:-2]\n",
    "    loc = params[-2]\n",
    "    scale = params[-1]\n",
    "\n",
    "    # Get sane start and end points of distribution\n",
    "    start = dist.ppf(0.01, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.01, loc=loc, scale=scale)\n",
    "    end = dist.ppf(0.99, *arg, loc=loc, scale=scale) if arg else dist.ppf(0.99, loc=loc, scale=scale)\n",
    "\n",
    "    # Build PDF and turn into pandas Series\n",
    "    x = np.linspace(start, end, size)\n",
    "    y = dist.pdf(x, loc=loc, scale=scale, *arg)\n",
    "    pdf = pd.Series(y, x)\n",
    "\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "trips = pd.read_csv('/home/emma/ual_model_workspace/spring-2019-models/notebooks-emma/HStrips_031219.csv', index_col = 0)\n",
    "\n",
    "#select people who make both home-school and school-home trips:\n",
    "tripsII = trips.groupby('HHPER').filter(lambda x: len(x) == 2)\n",
    "\n",
    "#make sure all home-school trip rows are listed first\n",
    "tripsIII = tripsII.sort_values(['HHPER','origin'])\n",
    "\n",
    "#move school-home trip info up into home-school trip rows\n",
    "tripsIII['school_dwell'] = tripsIII.groupby('HHPER', group_keys=False).origin_dwell.shift(-1)\n",
    "tripsIII['school_ST'] = tripsIII.groupby('HHPER', group_keys=False).origin_ST.shift(-1)\n",
    "tripsIII['SH_trip_ST'] = tripsIII.groupby('HHPER', group_keys=False).origin_ET.shift(-1)\n",
    "tripsIII['SH_trip_ET'] = tripsIII.groupby('HHPER', group_keys=False).trip_ET.shift(-1)\n",
    "tripsIII['SH_TT'] = tripsIII.groupby('HHPER', group_keys=False).TT.shift(-1)\n",
    "tripsIII['SH_mode'] = tripsIII.groupby('HHPER', group_keys=False).MODE.shift(-1)\n",
    "\n",
    "tripsIII = tripsIII.groupby('HHPER').first().reset_index()\n",
    "\n",
    "tripsIII.rename(columns = {'origin_dwell':'home_dwell','origin_ST':'home_ST','origin_ET':'HS_trip_ST',\n",
    "                           'trip_ET':'HS_trip_ET','TT':'HS_TT','MODE':'HS_mode','TOD':'HS_TOD'},inplace = True)\n",
    "\n",
    "#School arrival time TOD category\n",
    "tripsIII['School_arrival_TOD'] = pd.cut(tripsIII.HS_trip_ET, \n",
    "                             np.array([0, 3.0, 7.75, 8.5, 9.5, 15.0, 24.0]), \n",
    "                             labels = [6,1,2,3,4,5]).replace(6,5).astype(int)\n",
    "\n",
    "#School departure time TOD category\n",
    "tripsIII['School_departure_TOD'] = pd.cut(tripsIII.SH_trip_ST, \n",
    "                             np.array([0, 10, 12, 15, 17, 20, 24]), \n",
    "                             labels = [6,1,2,3,4,5]).replace(6, 5).astype(int)\n",
    "\n",
    "trips_final = tripsIII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting series for each TOD category\n",
    "TOD_cat = [1,2,3,4,5]\n",
    "\n",
    "TOD_series = dict()\n",
    "for x in range(1,6):\n",
    "    name_arrival = 'TOD_arrival_'+str(x)\n",
    "    name_departure = 'TOD_departure_'+str(x)\n",
    "    TOD_series[name_arrival] = trips_final.HS_trip_ET[trips_final.School_arrival_TOD == x]\n",
    "    TOD_series[name_departure] = trips_final.SH_trip_ST[trips_final.School_departure_TOD == x]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate distributions for each series \n",
    "TOD_best_distributions = dict()\n",
    "\n",
    "for k,v in TOD_series.items(): \n",
    "    best_fit = best_fit_distribution(v, bins=200, ax=None)\n",
    "    TOD_best_distributions[k] = best_fit\n",
    "\n",
    "# best_fit_distribution(TOD_series['TOD_arrival_1'], bins=200, ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving .plk file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a pkl file\n",
    "file_Name = \"/home/juan/activitysynth/activitysynth/configs/TOD_school_distributions.pkl\"\n",
    "fileObject = open(file_Name,'wb') \n",
    "dill.dump(TOD_best_distributions,fileObject)   \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TOD_arrival_1': ('pearson3',\n",
       "  (-2.1741709726462837, 6.910093584993785, 0.9130500736229582)),\n",
       " 'TOD_departure_1': ('pearson3',\n",
       "  (-2.334661440450379, 11.505235230760526, 0.5775541144183648)),\n",
       " 'TOD_arrival_2': ('chi',\n",
       "  (1.2722603822487915, 7.764701020800063, 0.3379469030503246)),\n",
       " 'TOD_departure_2': ('pearson3',\n",
       "  (-2.4947923251836954, 14.44289891242401, 0.6949257588180353)),\n",
       " 'TOD_arrival_3': ('chi',\n",
       "  (1.1267933578858025, 8.515530267168568, 0.4567291677046388)),\n",
       " 'TOD_departure_3': ('f',\n",
       "  (6.140448179796783,\n",
       "   5.12860067404247,\n",
       "   14.95076976922487,\n",
       "   0.5142603319944937)),\n",
       " 'TOD_arrival_4': ('chi',\n",
       "  (0.9276387555111918, 9.516666666666666, 2.5362791520006303)),\n",
       " 'TOD_departure_4': ('powerlognorm',\n",
       "  (143.27166513533655,\n",
       "   2.0807126677365115,\n",
       "   16.98060055651859,\n",
       "   167.9898861447187)),\n",
       " 'TOD_arrival_5': ('gumbel_l', (18.029208063456373, 1.0931053355051947)),\n",
       " 'TOD_departure_5': ('dgamma',\n",
       "  (0.6570762778899916, 20.999999999999996, 4.472119433950127))}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing pkl file\n",
    "fileObject = open(file_Name,'rb')  \n",
    "# load the object from the file into var b\n",
    "b = pickle.load(fileObject)  \n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
